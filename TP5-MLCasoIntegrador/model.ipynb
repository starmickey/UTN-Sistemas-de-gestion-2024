{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de Riesgo de Enfermedad Coronaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Preprocess data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Optimize model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Train model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Get metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datos_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesor:\n",
    "  def __init__(self, df):\n",
    "    self.df = df.copy()\n",
    "\n",
    "\n",
    "  # AUXILIARY FUNCTIONS\n",
    "  \n",
    "  def _fix_age(self, age):\n",
    "    if not isinstance(age, str):\n",
    "      return age\n",
    "    age = age.replace(\"Age \", \"\").replace(\" to \", \"-\").replace(\" or older\", \"-\")\n",
    "    return age\n",
    "\n",
    "  def _get_avg_from_num_str(self, age):\n",
    "    if not isinstance(age, str):\n",
    "      return age\n",
    "    strnums = age.split('-')\n",
    "    nums = [int(strnum) for strnum in strnums if strnum]\n",
    "    avg = sum(nums) / len(nums)\n",
    "    return int(avg)\n",
    "\n",
    "  def set_id_as_index(self):\n",
    "    self.df.set_index('id', inplace=True)\n",
    "\n",
    "  def fix_ages(self):\n",
    "    self.df[\"CategoriaDeEdad\"] = self.df[\"CategoriaDeEdad\"].apply(self._fix_age)\n",
    "\n",
    "\n",
    "  # REPLACE STRINGS WITH NUMBERS\n",
    "\n",
    "  def replace_yes_no(self, col):\n",
    "    self.df[col] = self.df[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "  def set_to_dummie(self, cols):\n",
    "    self.df = pd.get_dummies(self.df, columns=cols, dtype=float)\n",
    "\n",
    "  def set_salud_general_to_ordinal_nums(self):\n",
    "    self.df[\"SaludGeneral\"] = self.df[\"SaludGeneral\"].map({\n",
    "      'Excellent': 5,\n",
    "      'Very good' : 4,\n",
    "      'Good': 3, \n",
    "      'Fair': 2,\n",
    "      'Poor': 1\n",
    "    })\n",
    "\n",
    "  def set_categoria_de_edad_to_ordinal_nums(self):\n",
    "    self.df[\"CategoriaDeEdad\"] = self.df[\"CategoriaDeEdad\"].apply(self._get_avg_from_num_str)\n",
    "\n",
    "  def set_fumador_to_weights(self):\n",
    "    self.df[\"Fumador\"] = self.df[\"Fumador\"].map({\n",
    "      'Never smoked' : 0,\n",
    "      'No': 2,\n",
    "      'Former smoker' : 4,\n",
    "      'Yes': 5, \n",
    "      'Current smoker - now smokes some days': 7,\n",
    "      'Current smoker - now smokes every day': 10\n",
    "    })\n",
    "\n",
    "  def set_sex_to_boolean(self):\n",
    "    self.df[\"Sexo\"] = self.df[\"Sexo\"].map({\n",
    "      'Male': 1,\n",
    "      'Female' : 0,\n",
    "    })\n",
    "\n",
    "\n",
    "  # NAN HANDLING\n",
    "\n",
    "  def replace_nan_with_median(self, col_names):\n",
    "    for col_name in col_names:\n",
    "      col = self.df[col_name]\n",
    "      median = statistics.median(col.dropna())\n",
    "      self.df[col_name] = col.fillna(median)    \n",
    "\n",
    "  def drop_column(self, col):\n",
    "    self.df = self.df.drop(col, axis=1)\n",
    "\n",
    "\n",
    "  # CLEAN DATA\n",
    "\n",
    "  def clean_data(self):\n",
    "    self.df = self.df.rename(columns={'AccidenteCerebrovascular ': 'AccidenteCerebroVascular'})\n",
    "    self.fix_ages()\n",
    "    self.set_id_as_index()\n",
    "    self.replace_yes_no('ActividadFisica')\n",
    "    self.replace_yes_no('AccidenteCerebroVascular')\n",
    "    self.replace_yes_no('Asma')\n",
    "    self.replace_yes_no('CáncerDePiel')\n",
    "    self.replace_yes_no('Diabetes')\n",
    "    self.replace_yes_no('ConsumoDeAlcohol')\n",
    "    self.replace_yes_no('EnfermedadRenal')\n",
    "    self.replace_yes_no('DificultadParaCaminar')\n",
    "    self.set_to_dummie(['Sexo', 'Raza'])\n",
    "\n",
    "\n",
    "  # TRANSFORM FUNCTIONS\n",
    "\n",
    "  def transform_1(self):\n",
    "    self.clean_data()\n",
    "    self.set_salud_general_to_ordinal_nums()\n",
    "    self.set_categoria_de_edad_to_ordinal_nums()\n",
    "    self.drop_column(\"SaludFisica\")\n",
    "    self.drop_column(\"SaludMental\")\n",
    "    self.set_fumador_to_weights()\n",
    "    self.replace_nan_with_median(self.df.columns)\n",
    "  \n",
    "  def transform_2(self):\n",
    "    self.clean_data()\n",
    "    self.set_salud_general_to_ordinal_nums()\n",
    "    self.set_categoria_de_edad_to_ordinal_nums()\n",
    "    self.drop_column(\"SaludFisica\")\n",
    "    self.drop_column(\"SaludMental\")\n",
    "    self.set_fumador_to_weights()\n",
    "    self.df[\"IMC\"].dropna()\n",
    "    self.df[\"Fumador\"].dropna()\n",
    "    self.replace_nan_with_median(self.df.columns)\n",
    "\n",
    "  def transform_3(self):\n",
    "    self.df = self.df.rename(columns={'AccidenteCerebrovascular ': 'AccidenteCerebroVascular'})\n",
    "    self.fix_ages()\n",
    "    self.set_id_as_index()\n",
    "    self.replace_yes_no('ActividadFisica')\n",
    "    self.replace_yes_no('AccidenteCerebroVascular')\n",
    "    self.replace_yes_no('Asma')\n",
    "    self.replace_yes_no('CáncerDePiel')\n",
    "    self.replace_yes_no('Diabetes')\n",
    "    self.replace_yes_no('ConsumoDeAlcohol')\n",
    "    self.replace_yes_no('EnfermedadRenal')\n",
    "    self.replace_yes_no('DificultadParaCaminar')\n",
    "    self.set_to_dummie(['Sexo'])\n",
    "    self.drop_column(\"Raza\")\n",
    "    self.set_salud_general_to_ordinal_nums()\n",
    "    self.set_categoria_de_edad_to_ordinal_nums()\n",
    "    self.drop_column(\"SaludFisica\")\n",
    "    self.drop_column(\"SaludMental\")\n",
    "    self.set_fumador_to_weights()\n",
    "    self.df[\"IMC\"].dropna()\n",
    "    self.df[\"Fumador\"].dropna()\n",
    "    self.replace_nan_with_median(self.df.columns)\n",
    "\n",
    "  def get_df(self):\n",
    "    return self.df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scales preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStandardScaler:\n",
    "  def __init__(self, train):\n",
    "    self.scaler = StandardScaler()\n",
    "    self.scaler.fit(train)\n",
    "  \n",
    "  def transform(self, data):\n",
    "    return self.scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X and Y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXandY(data):\n",
    "  data = data.copy()\n",
    "  X = data.drop('EnfermedadCoronaria', axis = 1)\n",
    "  y = data[[\"EnfermedadCoronaria\"]]\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "  model.fit(X_train, y_train.values.ravel())\n",
    "  y_pred = model.predict(X_test)\n",
    "  y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "  \n",
    "  accuracy = accuracy_score(y_test,y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "  \n",
    "  metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "    'AUC-ROC': auc_roc\n",
    "  }\n",
    "  \n",
    "  return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedFoldStrategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(model, X, y, n_splits=5):\n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize lists to store metrics for each fold\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    auc_roc_scores = []\n",
    "    fit_times = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.loc[train_index, :], X.loc[test_index, :]\n",
    "        y_train, y_test = y.loc[train_index, :], y.loc[test_index, :]\n",
    "\n",
    "        # Apply standard scaling\n",
    "        standardScaler = CustomStandardScaler(X)\n",
    "        X_train = standardScaler.transform(X_train)\n",
    "        X_test  = standardScaler.transform(X_test)\n",
    "        \n",
    "        # Measure time to fit the model\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate fitting time\n",
    "        fit_time = end_time - start_time\n",
    "        fit_times.append(fit_time)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test,y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Append metrics to the lists\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate the mean and standard deviation of each metric\n",
    "    metrics = {\n",
    "        'Accuracy': (np.mean(accuracy_scores), np.std(accuracy_scores)),\n",
    "        'Precision': (np.mean(precision_scores), np.std(precision_scores)),\n",
    "        'Recall': (np.mean(recall_scores), np.std(recall_scores)),\n",
    "        'F1 Score': (np.mean(f1_scores), np.std(f1_scores)),\n",
    "        'AUC-ROC': (np.mean(auc_roc_scores), np.std(auc_roc_scores)),\n",
    "        'Fit Time': (np.mean(fit_times), np.std(fit_times))\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models, X, y, verbose = 0):\n",
    "  results = []\n",
    "\n",
    "  for name, model in models.items():\n",
    "    metrics = evaluate_model_cv(model, X, y)\n",
    "    result = {\n",
    "      'Model': name,\n",
    "      'Accuracy (Mean)': metrics['Accuracy'][0],\n",
    "      'Accuracy (Std)': metrics['Accuracy'][1],\n",
    "      'Precision (Mean)': metrics['Precision'][0],\n",
    "      'Precision (Std)': metrics['Precision'][1],\n",
    "      'Recall (Mean)': metrics['Recall'][0],\n",
    "      'Recall (Std)': metrics['Recall'][1],\n",
    "      'F1 Score (Mean)': metrics['F1 Score'][0],\n",
    "      'F1 Score (Std)': metrics['F1 Score'][1],\n",
    "      'AUC-ROC (Mean)': metrics['AUC-ROC'][0],\n",
    "      'AUC-ROC (Std)': metrics['AUC-ROC'][1],\n",
    "      'Fit Time (Mean)': metrics['Fit Time'][0],\n",
    "      'Fit Time (Std)': metrics['Fit Time'][1]\n",
    "    }\n",
    "    if verbose:\n",
    "      print(result)\n",
    "    results.append(result)\n",
    "  \n",
    "  return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, verbose = 0):\n",
    "  X, y = getXandY(data)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "  # Apply standard scaling\n",
    "  standardScaler = CustomStandardScaler(X)\n",
    "  X_train = standardScaler.transform(X_train)\n",
    "  # Measure time to fit the model\n",
    "  model.fit(X_train, y_train.values.ravel())\n",
    "  if verbose:\n",
    "    X_test = standardScaler.transform(X_test)\n",
    "    print(evaluate_model(model, X_train, y_train, X_test, y_test))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParams(X_train, y_train, model, param_grid):\n",
    "  # Initialize the grid search model\n",
    "  grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "  # Fit the grid search model\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  # Print best params\n",
    "  print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# SAMPLE\n",
    "# param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'saga']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Preprocesor(df)\n",
    "p1.transform_1()\n",
    "data_1 = p1.get_df()\n",
    "\n",
    "X_1, y_1 = getXandY(data)\n",
    "standardScaler_1 = CustomStandardScaler(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = Preprocesor(df)\n",
    "p2.transform_2()\n",
    "data_2 = p2.get_df()\n",
    "\n",
    "X_2, y_2 = getXandY(data)\n",
    "standardScaler_2 = CustomStandardScaler(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = Preprocesor(df)\n",
    "p3.transform_3()\n",
    "data_3 = p3.get_df()\n",
    "\n",
    "X_3, y_3 = getXandY(data)\n",
    "standardScaler_3 = CustomStandardScaler(X_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegression_1 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "logisticRegression_2 = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "\n",
    "logisticRegression_3 = LogisticRegression(C=0.01, solver=\"liblinear\", random_state=42, max_iter=1000)\n",
    "\n",
    "logisticRegression_4 = LogisticRegression(class_weight='balanced', C=0.01, solver=\"liblinear\",\n",
    "                                          random_state=42, max_iter=1000)\n",
    "\n",
    "logisticRegression_5 = LogisticRegression(class_weight='balanced', C=0.01, solver=\"liblinear\",\n",
    "                                          penalty=\"l1\", random_state=42, max_iter=1000)\n",
    "\n",
    "logisticRegression_6 = LogisticRegression(class_weight='balanced', C=0.01, solver=\"lbfgs\",\n",
    "                                          random_state=42, max_iter=1000)\n",
    "              \n",
    "logisticRegression_7 = LogisticRegression(class_weight='balanced', C=0.001, solver=\"liblinear\",\n",
    "                                          random_state=42, max_iter=1000)                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': (0.5282212896201022, 0.006865079920724799),\n",
       " 'Recall': (0.07162081695843754, 0.0020386341718738256),\n",
       " 'F1 Score': (0.12612401075156313, 0.003135026580383677),\n",
       " 'AUC-ROC': (0.8376185270139803, 0.0017678177991564982)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = logisticRegression_3\n",
    "data = data_1\n",
    "\n",
    "# Get Crossed Validation datasets\n",
    "X, y = getXandY(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply standard scaling\n",
    "standardScaler = CustomStandardScaler(X)\n",
    "X_train_scaled = standardScaler.transform(X_train)\n",
    "X_test_scaled  = standardScaler.transform(X_test)\n",
    "\n",
    "# evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "evaluate_model_cv(model, X, y, n_splits=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'Logistic Regression 1', 'Precision (Mean)': 0.1840236487723898, 'Precision (Std)': 0.0006686890539157039, 'Recall (Mean)': 0.7780159580802668, 'Recall (Std)': 0.005667990625645661, 'F1 Score (Mean)': 0.297643756185946, 'F1 Score (Std)': 0.0011640508001936147, 'AUC-ROC (Mean)': 0.8384853562476128, 'AUC-ROC (Std)': 0.0017524458122455645}\n",
      "{'Model': 'Logistic Regression 2', 'Precision (Mean)': 0.5263322972045271, 'Precision (Std)': 0.006330585644712654, 'Recall (Mean)': 0.07424080028581637, 'Recall (Std)': 0.002256561968765422, 'F1 Score (Mean)': 0.13010683165371725, 'F1 Score (Std)': 0.0033752742842296855, 'AUC-ROC (Mean)': 0.838113572378874, 'AUC-ROC (Std)': 0.0017851364307249055}\n",
      "                   Model  Precision (Mean)  Precision (Std)  Recall (Mean)  \\\n",
      "0  Logistic Regression 1          0.184024         0.000669       0.778016   \n",
      "1  Logistic Regression 2          0.526332         0.006331       0.074241   \n",
      "\n",
      "   Recall (Std)  F1 Score (Mean)  F1 Score (Std)  AUC-ROC (Mean)  \\\n",
      "0      0.005668         0.297644        0.001164        0.838485   \n",
      "1      0.002257         0.130107        0.003375        0.838114   \n",
      "\n",
      "   AUC-ROC (Std)  \n",
      "0       0.001752  \n",
      "1       0.001785  \n"
     ]
    }
   ],
   "source": [
    "data = data_1\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'Logistic Regression 1': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "    'Logistic Regression 2': LogisticRegression(random_state=42, max_iter=1000),\n",
    "}\n",
    "\n",
    "X, y = getXandY(data)\n",
    "\n",
    "# Compare the models\n",
    "comparison_df = compare_models(models, X, y, 1)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LR 2', 'Accuracy (Mean)': 0.7470684493292525, 'Accuracy (Std)': 0.0011296632264012265, 'Precision (Mean)': 0.1840236487723898, 'Precision (Std)': 0.0006686890539157039, 'Recall (Mean)': 0.7780159580802668, 'Recall (Std)': 0.005667990625645661, 'F1 Score (Mean)': 0.297643756185946, 'F1 Score (Std)': 0.0011640508001936147, 'AUC-ROC (Mean)': 0.8384853562476128, 'AUC-ROC (Std)': 0.0017524458122455645, 'Fit Time (Mean)': 1.93329176902771, 'Fit Time (Std)': 0.15217076033949264}\n",
      "{'Model': 'LR 3', 'Accuracy (Mean)': 0.9316443775616088, 'Accuracy (Std)': 0.00012880417582060956, 'Precision (Mean)': 0.528022599747968, 'Precision (Std)': 0.007328708705631654, 'Recall (Mean)': 0.07290699059187805, 'Recall (Std)': 0.002225934784795727, 'F1 Score (Mean)': 0.12810421141452608, 'F1 Score (Std)': 0.0033737782975798865, 'AUC-ROC (Mean)': 0.8381788572443535, 'AUC-ROC (Std)': 0.0017824239960852494, 'Fit Time (Mean)': 3.1697707653045653, 'Fit Time (Std)': 0.10357516411818854}\n",
      "{'Model': 'LR 4', 'Accuracy (Mean)': 0.7469125810796309, 'Accuracy (Std)': 0.0011386879549406172, 'Precision (Mean)': 0.18397141092560784, 'Precision (Std)': 0.0006575583923760966, 'Recall (Mean)': 0.7783255924735024, 'Recall (Std)': 0.005745331522153439, 'F1 Score (Mean)': 0.2975980141512455, 'F1 Score (Std)': 0.0011542215086756938, 'AUC-ROC (Mean)': 0.838491866183914, 'AUC-ROC (Std)': 0.0017527454326951727, 'Fit Time (Mean)': 2.8806760787963865, 'Fit Time (Std)': 0.11656763893656664}\n",
      "  Model  Accuracy (Mean)  Accuracy (Std)  Precision (Mean)  Precision (Std)  \\\n",
      "0  LR 2         0.747068        0.001130          0.184024         0.000669   \n",
      "1  LR 3         0.931644        0.000129          0.528023         0.007329   \n",
      "2  LR 4         0.746913        0.001139          0.183971         0.000658   \n",
      "\n",
      "   Recall (Mean)  Recall (Std)  F1 Score (Mean)  F1 Score (Std)  \\\n",
      "0       0.778016      0.005668         0.297644        0.001164   \n",
      "1       0.072907      0.002226         0.128104        0.003374   \n",
      "2       0.778326      0.005745         0.297598        0.001154   \n",
      "\n",
      "   AUC-ROC (Mean)  AUC-ROC (Std)  Fit Time (Mean)  Fit Time (Std)  \n",
      "0        0.838485       0.001752         1.933292        0.152171  \n",
      "1        0.838179       0.001782         3.169771        0.103575  \n",
      "2        0.838492       0.001753         2.880676        0.116568  \n"
     ]
    }
   ],
   "source": [
    "data = data_1\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'LR 2': logisticRegression_2,\n",
    "    'LR 3': logisticRegression_3,\n",
    "    'LR 4': logisticRegression_4,\n",
    "}\n",
    "\n",
    "X, y = getXandY(data)\n",
    "\n",
    "# Compare the models\n",
    "comparison_df = compare_models(models, X, y, 1)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': (0.18397141092560784, 0.0006575583923760966), 'Recall': (0.7783255924735024, 0.005745331522153439), 'F1 Score': (0.2975980141512455, 0.0011542215086756938), 'AUC-ROC': (0.838491866183914, 0.0017527454326951727), 'Fit Time': (2.987709665298462, 0.06945670760275129)}\n",
      "{'Precision': (0.18397141092560784, 0.0006575583923760966), 'Recall': (0.7783255924735024, 0.005745331522153439), 'F1 Score': (0.2975980141512455, 0.0011542215086756938), 'AUC-ROC': (0.838491866183914, 0.0017527454326951727), 'Fit Time': (3.643292236328125, 0.7537846196978392)}\n"
     ]
    }
   ],
   "source": [
    "X_1, y_1 = getXandY(data_1)\n",
    "X_2, y_2 = getXandY(data_2)\n",
    "\n",
    "model = logisticRegression_4\n",
    "\n",
    "print(evaluate_model_cv(model, X_1, y_1, n_splits=5))\n",
    "print(evaluate_model_cv(model, X_2, y_2, n_splits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'LR 4', 'Accuracy (Mean)': 0.7469125810796309, 'Accuracy (Std)': 0.0011386879549406172, 'Precision (Mean)': 0.18397141092560784, 'Precision (Std)': 0.0006575583923760966, 'Recall (Mean)': 0.7783255924735024, 'Recall (Std)': 0.005745331522153439, 'F1 Score (Mean)': 0.2975980141512455, 'F1 Score (Std)': 0.0011542215086756938, 'AUC-ROC (Mean)': 0.838491866183914, 'AUC-ROC (Std)': 0.0017527454326951727, 'Fit Time (Mean)': 3.0690026760101317, 'Fit Time (Std)': 0.11631498425902}\n",
      "{'Model': 'LR 5', 'Accuracy (Mean)': 0.7466517069381678, 'Accuracy (Std)': 0.0011833750893363256, 'Precision (Mean)': 0.18385689424684143, 'Precision (Std)': 0.0007381838095883737, 'Recall (Mean)': 0.778659044896987, 'Recall (Std)': 0.00581794088793942, 'F1 Score (Mean)': 0.2974724890654553, 'F1 Score (Std)': 0.0012594966378245923, 'AUC-ROC (Mean)': 0.8384899023240429, 'AUC-ROC (Std)': 0.0017545328576010818, 'Fit Time (Mean)': 10.835013103485107, 'Fit Time (Std)': 2.965923943297715}\n",
      "{'Model': 'LR 6', 'Accuracy (Mean)': 0.7471307966048736, 'Accuracy (Std)': 0.0011226314007115662, 'Precision (Mean)': 0.18404170639548534, 'Precision (Std)': 0.0006737030064979675, 'Recall (Mean)': 0.7778730498987734, 'Recall (Std)': 0.005633052274851602, 'F1 Score (Mean)': 0.29765694342879934, 'F1 Score (Std)': 0.0011692919030119409, 'AUC-ROC (Mean)': 0.8384901133168071, 'AUC-ROC (Std)': 0.001752449311580658, 'Fit Time (Mean)': 2.118425226211548, 'Fit Time (Std)': 0.24508370806025642}\n",
      "{'Model': 'LR 7', 'Accuracy (Mean)': 0.7471307966048736, 'Accuracy (Std)': 0.0011226314007115662, 'Precision (Mean)': 0.18404170639548534, 'Precision (Std)': 0.0006737030064979675, 'Recall (Mean)': 0.7778730498987734, 'Recall (Std)': 0.005633052274851602, 'F1 Score (Mean)': 0.29765694342879934, 'F1 Score (Std)': 0.0011692919030119409, 'AUC-ROC (Mean)': 0.8384901133168071, 'AUC-ROC (Std)': 0.001752449311580658, 'Fit Time (Mean)': 2.2187169551849366, 'Fit Time (Std)': 0.222981473613287}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "l1_ratio must be specified when penalty is elasticnet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m X, y \u001b[38;5;241m=\u001b[39m getXandY(data)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compare the models\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(comparison_df)\n",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m, in \u001b[0;36mcompare_models\u001b[1;34m(models, X, y, verbose)\u001b[0m\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 5\u001b[0m   metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m   result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: name,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy (Mean)\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFit Time (Std)\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFit Time\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m   }\n\u001b[0;32m     21\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[1;32mIn[29], line 25\u001b[0m, in \u001b[0;36mevaluate_model_cv\u001b[1;34m(model, X, y, n_splits)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Measure time to fit the model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate fitting time\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Micaela Estrella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micaela Estrella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_ratio parameter is only used when penalty is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melasticnet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(penalty=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty)\n\u001b[0;32m   1179\u001b[0m     )\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melasticnet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_ratio must be specified when penalty is elasticnet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:  \u001b[38;5;66;03m# default values\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: l1_ratio must be specified when penalty is elasticnet."
     ]
    }
   ],
   "source": [
    "data = data_1\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'LR 4':  logisticRegression_4,\n",
    "    'LR 5':  logisticRegression_5,\n",
    "    'LR 6':  logisticRegression_6,\n",
    "}\n",
    "\n",
    "X, y = getXandY(data)\n",
    "\n",
    "# Compare the models\n",
    "comparison_df = compare_models(models, X, y, 1)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Accuracy (Mean)  Accuracy (Std)  Precision (Mean)  Precision (Std)  \\\n",
      "0  LR 4         0.746913        0.001139          0.183971         0.000658   \n",
      "1  LR 7         0.745590        0.001104          0.183467         0.000649   \n",
      "\n",
      "   Recall (Mean)  Recall (Std)  F1 Score (Mean)  F1 Score (Std)  \\\n",
      "0       0.778326      0.005745         0.297598        0.001154   \n",
      "1       0.780517      0.005669         0.297097        0.001144   \n",
      "\n",
      "   AUC-ROC (Mean)  AUC-ROC (Std)  Fit Time (Mean)  Fit Time (Std)  \n",
      "0        0.838492       0.001753         3.823942        0.800882  \n",
      "1        0.838538       0.001752         6.001464        2.416731  \n"
     ]
    }
   ],
   "source": [
    "data = data_1\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'LR 4':  logisticRegression_4,\n",
    "    'LR 7':  logisticRegression_7,\n",
    "}\n",
    "\n",
    "X, y = getXandY(data)\n",
    "\n",
    "# Compare the models\n",
    "comparison_df = compare_models(models, X, y)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get submit csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df = pd.read_csv('./nuevas_instancias_clasificar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.7444530126280442, 'Precision': 0.1824545725350015, 'Recall': 0.7769145394006659, 'F1 Score': 0.2955100563881434, 'AUC-ROC': 0.8357840715242532}\n"
     ]
    }
   ],
   "source": [
    "# Set custom imputs\n",
    "model = logisticRegression_5\n",
    "data = data_3\n",
    "standardScaler = standardScaler_3\n",
    "\n",
    "# Create and transform validation data\n",
    "v = Preprocesor(validate_df)\n",
    "v.transform_3()\n",
    "validate_data = v.get_df()\n",
    "validate_data = standardScaler.transform(validate_data)\n",
    "\n",
    "# Train model\n",
    "train_model(model, data, verbose=1)\n",
    "# Get prediction\n",
    "validate_prediction = model.predict(validate_data)\n",
    "\n",
    "# Format results dataframe\n",
    "prediction_df = pd.DataFrame(validate_prediction)\n",
    "prediction_df.columns = ['Predicted']\n",
    "prediction_df.index.names = ['id']\n",
    "# Create csv\n",
    "date = datetime.datetime.now().strftime(\"%d-%Y-%I-%M-%p-%B\")\n",
    "filename = 'prediction-results/result-' + date + '.csv'\n",
    "prediction_df.to_csv(filename,sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LR 4:** 0.76143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_2\n",
    "\n",
    "logisticRegression_4 = LogisticRegression(class_weight='balanced', C=0.01, solver=\"liblinear\",\n",
    "                                          random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LR 6:** 0.76155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_2\n",
    "\n",
    "logisticRegression_6 = LogisticRegression(class_weight='balanced', C=0.01, solver=\"lbfgs\",\n",
    "                                          random_state=42, max_iter=1000)\n",
    "\n",
    "\n",
    "logisticRegression_5 = LogisticRegression(class_weight='balanced', C=0.01, solver=\"liblinear\",\n",
    "                                          penalty=\"l1\", random_state=42, max_iter=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
